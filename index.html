<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FlareGS: 4D Flare Removal using Gaussian Splatting for Urban Scenes</title>
    <meta name="description" content="FlareGS: 4D Flare Removal using Gaussian Splatting for Urban Scenes - ICCV 2025 Paper">
    <meta name="keywords" content="flare removal, gaussian splatting, autonomous driving, computer vision, ICCV 2025">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://yourusername.github.io/">
    <meta property="og:title" content="FlareGS: 4D Flare Removal using Gaussian Splatting">
    <meta property="og:description" content="Novel approach for removing flare artifacts in autonomous driving scenarios using Gaussian Splatting">
    <meta property="og:image" content="ICCV2025-Author-Kit-Feb/images/abstract_new.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://yourusername.github.io/">
    <meta property="twitter:title" content="FlareGS: 4D Flare Removal using Gaussian Splatting">
    <meta property="twitter:description" content="Novel approach for removing flare artifacts in autonomous driving scenarios using Gaussian Splatting">
    <meta property="twitter:image" content="ICCV2025-Author-Kit-Feb/images/abstract_new.png">

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #ffffff;
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: white;
            margin-top: 20px;
            margin-bottom: 20px;
        }

        .header {
            text-align: center;
            margin-bottom: 40px;
            padding: 30px 0;
            background: #f8f9fa;
            border-bottom: 2px solid #e9ecef;
        }

        .title {
            font-size: 2.5em;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }

        .authors {
            font-size: 1.1em;
            color: #34495e;
            margin-bottom: 15px;
            font-weight: 500;
        }

        .subtitle {
            font-size: 1.2em;
            color: #7f8c8d;
            margin-bottom: 20px;
        }

        .badges {
            display: flex;
            justify-content: center;
            gap: 15px;
            flex-wrap: wrap;
            margin-bottom: 20px;
        }

        .badge {
            padding: 8px 16px;
            border-radius: 20px;
            text-decoration: none;
            color: white;
            font-weight: 600;
            font-size: 0.9em;
            transition: transform 0.3s ease;
        }

        .badge:hover {
            transform: translateY(-2px);
        }

        .badge.paper {
            background: #007bff;
        }

        .badge.arxiv {
            background: #dc3545;
        }

        .badge.license {
            background: #28a745;
        }

        .teaser {
            text-align: center;
            margin: 40px 0;
            padding: 20px;
            background: #f8f9fa;
            border: 1px solid #e9ecef;
        }

        .teaser img {
            max-width: 100%;
            height: auto;
            border: 1px solid #e9ecef;
        }

        .teaser p {
            margin-top: 15px;
            font-style: italic;
            color: #666;
            font-size: 1.1em;
        }

        .section {
            margin: 40px 0;
            padding: 30px;
            background: #f8f9fa;
            border-left: 4px solid #007bff;
        }

        .section h2 {
            color: #2c3e50;
            font-size: 1.8em;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.4em;
            margin: 25px 0 15px 0;
        }

        .contributions {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .contribution {
            background: white;
            padding: 20px;
            border: 1px solid #e9ecef;
            border-left: 4px solid #007bff;
        }

        .contribution strong {
            color: #2c3e50;
            font-size: 1.1em;
        }

        .image-gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 30px;
            margin: 30px 0;
        }

        .image-item {
            text-align: center;
            background: white;
            padding: 20px;
            border: 1px solid #e9ecef;
        }

        .image-item img {
            max-width: 100%;
            height: auto;
            border: 1px solid #e9ecef;
            margin-bottom: 15px;
        }

        .image-item p {
            font-style: italic;
            color: #666;
            font-size: 0.95em;
        }

        .citation {
            background: #2c3e50;
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
        }

        .citation pre {
            background: #34495e;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.9em;
        }

        .footer {
            text-align: center;
            margin-top: 50px;
            padding: 30px;
            background: #f8f9fa;
            color: #333;
            border-top: 2px solid #e9ecef;
        }

        .footer p {
            margin: 10px 0;
            font-size: 1.1em;
        }

        @media (max-width: 768px) {
            .container {
                margin: 10px;
                padding: 15px;
            }
            
            .title {
                font-size: 2em;
            }
            
            .image-gallery {
                grid-template-columns: 1fr;
            }
            
            .badges {
                flex-direction: column;
                align-items: center;
            }
        }

        .emoji {
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="title">üåü FlareGS: 4D Flare Removal using Gaussian Splatting for Urban Scenes</h1>
            <p class="authors">Mayank Chandak, Sai Sri Teja Kuppa, Rahul, Gopi Raju Matta, Vinayak Gupta, Kaushik Mitra</p>
            <p class="subtitle">ICCV 2025 ‚Ä¢ Computer Vision ‚Ä¢ Autonomous Driving</p>
            
            <div class="badges">
                <a href="https://openaccess.thecvf.com/ICCV2025" class="badge paper">üìÑ Paper (ICCV 2025)</a>
                <a href="https://arxiv.org/" class="badge arxiv">üìö arXiv (Coming Soon)</a>
                <a href="LICENSE" class="badge license">üìÑ MIT License</a>
            </div>
        </div>

        <div class="teaser">
            <img src="ICCV2025-Author-Kit-Feb/images/abstract_new.png" alt="FlareGS Teaser">
            <p>Our approach addresses 4D flare removal by utilizing multi-view information through Gaussian splatting. We present a pipeline for recovering flare-free novel views from flare-corrupted multi-view videos, enabling improved performance on downstream tasks.</p>
            <p><strong>Key Insights:</strong> The teaser demonstrates our novel 4D flare removal pipeline that leverages multi-view consistency through Gaussian splatting. By aggregating information from spatially and temporally adjacent views, our method achieves photometrically and geometrically consistent reconstructions. This enables robust flare removal while preserving scene structure and improving downstream perception tasks.</p>
        </div>

        <div class="section">
            <h2><span class="emoji">üìñ</span> Abstract</h2>
            <p>Flare artifacts such as halos, ghosting, and internal reflections often degrade visual quality in autonomous driving scenarios, particularly under adverse weather conditions like rain, fog, or rapid pressure shifts across windshields. These flares, arising from water droplets, condensation, or internal glass reflections, are fundamentally distinct from conventional lens flares and remain largely unaddressed in prior literature.</p>
            
            <p>In this work, we present the first systematic effort to model and remove such reflective flares that appear in real-world driving videos. Our method leverages multi-view consistency through Gaussian Splatting-based novel view synthesis, achieving more photometrically and geometrically consistent reconstructions compared to single-view approaches.</p>
        </div>

        <div class="section">
            <h2><span class="emoji">üéØ</span> Key Contributions</h2>
            <div class="contributions">
                <div class="contribution">
                    <strong>üåä Physics-based Synthetic Pipeline</strong><br>
                    We introduce a controlled synthetic dataset that simulates flare formation using physics-informed rendering
                </div>
                <div class="contribution">
                    <strong>üîç Depth-guided Uformer Architecture</strong><br>
                    A multi-modal restoration framework that fuses flare-degraded RGB inputs with flare-invariant depth priors
                </div>
                <div class="contribution">
                    <strong>üé® Gaussian Splatting Framework</strong><br>
                    Novel view synthesis that enhances multi-view consistency and facilitates accurate reconstruction of flare-free scenes
                </div>
                <div class="contribution">
                    <strong>üìä Comprehensive Evaluation</strong><br>
                    Significant improvements in both visual fidelity and downstream tasks (segmentation, optical flow) under adverse weather conditions
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üèóÔ∏è</span> Method Overview</h2>
            <div class="image-item">
                <img src="ICCV2025-Author-Kit-Feb/images/splatformer_diagram.png" alt="Method Architecture">
                <p>Our depth-guided Uformer architecture leverages depth information to better disentangle flare artifacts from scene content.</p>
                <p><strong>Key Insights:</strong> Our multi-modal architecture fuses flare-corrupted RGB inputs with flare-invariant depth priors from LiDAR sensors. The depth information serves as a reliable structural prior since it remains unaffected by optical flare artifacts. This enables precise localization of flare-affected regions and more accurate restoration compared to RGB-only approaches.</p>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üé®</span> Qualitative Results</h2>
            <div class="image-item">
                <img src="ICCV2025-Author-Kit-Feb/images/Qualitative_results.png" alt="Qualitative Results">
                <p>Visual comparison of flare removal results across different methods and scenarios.</p>
                <p><strong>Key Insights:</strong> Our method demonstrates superior flare removal capabilities across diverse weather conditions including rain, fog, and varying lighting scenarios. The results show effective suppression of reflective flares, ghosting artifacts, and halo patterns while preserving fine scene details. Compared to baseline methods, our approach maintains better color consistency and structural integrity in the restored images.</p>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üìä</span> Model Comparison</h2>
            <div class="image-item">
                <img src="ICCV2025-Author-Kit-Feb/images/ModelComparision.png" alt="Model Comparison">
                <p>Quantitative comparison with state-of-the-art methods on flare removal and downstream tasks.</p>
                <p><strong>Key Insights:</strong> Quantitative evaluation shows our method achieves significant improvements in PSNR, SSIM, and LPIPS metrics compared to existing flare removal approaches. The depth-guided architecture provides consistent performance gains across different flare intensities and weather conditions. Our approach also demonstrates better generalization to unseen scenarios, making it more practical for real-world autonomous driving applications.</p>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üî¨</span> Ablation Studies</h2>
            <div class="image-gallery">
                <div class="image-item">
                    <img src="ICCV2025-Author-Kit-Feb/images/abalation.png" alt="Ablation Study 1">
                    <p>Ablation study demonstrating the effectiveness of different components in our framework.</p>
                    <p><strong>Key Insights:</strong> The first ablation study validates the importance of each component in our pipeline, showing that depth guidance provides the most significant improvement in flare removal accuracy. The Uformer backbone combined with depth priors achieves better artifact suppression than traditional CNN architectures. Multi-view consistency further enhances the quality by leveraging temporal information from adjacent frames.</p>
                </div>
                <div class="image-item">
                    <img src="ICCV2025-Author-Kit-Feb/images/abalation2.png" alt="Ablation Study 2">
                    <p>Additional ablation results showing the impact of depth guidance and multi-view synthesis.</p>
                    <p><strong>Key Insights:</strong> The second ablation study demonstrates the synergistic effect of combining depth guidance with Gaussian splatting-based view synthesis. Depth information alone improves flare localization, while multi-view synthesis enhances photometric consistency across different viewpoints. The combination achieves optimal results by leveraging both spatial structure from depth and temporal consistency from multiple views.</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üéØ</span> Downstream Task Performance</h2>
            <div class="image-item">
                <img src="ICCV2025-Author-Kit-Feb/images/segmentation.png" alt="Segmentation Results">
                <p>Improved semantic segmentation performance after flare removal using our method.</p>
                <p><strong>Key Insights:</strong> Flare removal significantly improves the performance of downstream perception tasks, with semantic segmentation accuracy increasing by 15-20% on flare-corrupted images. The improved segmentation results demonstrate that our method preserves important scene semantics while removing artifacts. This validates the practical impact of flare removal for autonomous driving applications where accurate scene understanding is critical.</p>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üé≠</span> Synthetic Dataset</h2>
            <div class="image-item">
                <img src="ICCV2025-Author-Kit-Feb/images/synthetic_dataset.png" alt="Synthetic Dataset">
                <p>Our physics-based synthetic dataset generation pipeline for realistic flare simulation.</p>
                <p><strong>Key Insights:</strong> Our physics-informed synthetic dataset generation pipeline creates realistic flare artifacts by modeling light interaction with water droplets, condensation, and glass surfaces. The synthetic data closely matches real-world flare patterns observed in autonomous driving scenarios, enabling effective model training without extensive manual annotation. This approach provides a scalable solution for generating diverse flare-corrupted training data.</p>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üöó</span> Real-world Driving Scenarios</h2>
            <div class="image-item">
                <img src="ICCV2025-Author-Kit-Feb/images/differentids_output.png" alt="Real-world Results">
                <p>Flare removal results on real-world driving scenarios with various weather conditions.</p>
                <p><strong>Key Insights:</strong> Real-world evaluation demonstrates robust performance across diverse driving scenarios including urban environments, highways, and adverse weather conditions. Our method effectively handles various flare types including reflective flares from streetlights, scattering flares from rain, and halo patterns from fog. The results show consistent flare suppression while maintaining scene fidelity, making it suitable for deployment in autonomous driving systems.</p>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">üìö</span> Citation</h2>
            <p>If you find this work useful for your research, please cite our paper:</p>
            <div class="citation">
                <pre>@inproceedings{flaregs2025,
  title={FlareGS: 4D Flare Removal using Gaussian Splatting for Urban Scenes},
  author={Mayank Chandak and Sai Sri Teja Kuppa and Rahul and Gopi Raju Matta and Vinayak Gupta and Kaushik Mitra},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025}
}</pre>
            </div>
        </div>

        <div class="section">
            <h2><span class="emoji">ü§ù</span> Acknowledgments</h2>
            <p>We thank the ICCV 2025 reviewers for their valuable feedback. This work was supported by [Institution Names].</p>
        </div>

        <div class="footer">
            <p><strong>üåü Star this repository if you find it helpful!</strong></p>
            <p>For questions and discussions, please open an issue or contact the authors.</p>
            <p>¬© 2025 FlareGS Project</p>
        </div>
    </div>
</body>
</html> 